defmodule RsolvWeb.Api.V1.VulnerabilityValidationControllerWithCache do
  @moduledoc """
  Enhanced vulnerability validation controller with robust false positive caching.
  Integrates the new ValidationCache system for improved performance.
  """
  
  use RsolvWeb, :controller

  alias Rsolv.ValidationCache
  alias Rsolv.Customers.ForgeAccount
  alias RsolvWeb.Telemetry.ValidationTelemetry
  alias RsolvWeb.Api.V1.FilePathClassifier
  alias RsolvWeb.Api.V1.SafePatternDetector

  require Logger

  plug RsolvWeb.Plugs.ApiAuthentication
  
  def validate(conn, params) do
    start_time = System.monotonic_time(:millisecond)
    customer = conn.assigns.customer

    with forge_result <- get_forge_account_or_test(customer),
         {:ok, vulnerabilities} <- validate_request(params) do
      
      # Process vulnerabilities with cache
      repository = params["repository"] || "unknown/repo"
      files = params["files"] || %{}
      
      # Extract forge ID from result (either real forge account or test ID)
      # Convert to string to support both integer IDs and string test IDs
      forge_id = case forge_result do
        {:ok, forge_account} -> to_string(forge_account.id)
        {:test, test_id} -> test_id
      end
      
      {validated_results, cache_stats} = process_with_cache(
        forge_id,
        repository,
        vulnerabilities,
        files
      )
      
      # Calculate overall stats
      stats = calculate_stats(validated_results)
      
      # Build response with cache metadata
      response = %{
        "validated" => validated_results,
        "stats" => Map.merge(stats, %{
          "cacheHits" => cache_stats.hits,
          "cacheMisses" => cache_stats.misses  
        }),
        "cache_stats" => build_cache_stats(cache_stats)
      }
      
      # Emit telemetry events (convert to atom keys for telemetry)
      telemetry_response = %{
        validated: validated_results,
        stats: Map.merge(stats, %{
          "cacheHits" => cache_stats.hits,
          "cacheMisses" => cache_stats.misses  
        })
      }
      ValidationTelemetry.emit_validation_request(start_time, telemetry_response, cache_stats.hits > 0)
      ValidationTelemetry.emit_false_positive(telemetry_response, customer.id)
      
      json(conn, response)
    else
      {:error, :invalid_request} = error ->
        ValidationTelemetry.emit_validation_error(start_time, error)
        conn
        |> put_status(400)
        |> json(%{error: "Invalid request format"})
    end
  end

  defp get_forge_account(customer) do
    import Ecto.Query
    
    # Query for forge accounts belonging to this customer
    query = from fa in ForgeAccount,
      where: fa.customer_id == ^customer.id,
      limit: 1
    
    case Rsolv.Repo.one(query) do
      nil -> {:error, :no_forge_account}
      forge_account -> {:ok, forge_account}
    end
  end
  
  defp get_forge_account_or_test(customer) do
    case get_forge_account(customer) do
      {:ok, forge_account} -> 
        {:ok, forge_account}
      {:error, :no_forge_account} ->
        # For test/demo accounts without forge accounts, use a synthetic test ID
        # This allows testing without requiring GitHub integration
        Logger.info("Customer #{customer.id} has no forge account, using test mode")
        {:test, "test-forge-#{customer.id}"}
    end
  end
  
  defp validate_request(%{"vulnerabilities" => vulnerabilities}) 
       when is_list(vulnerabilities) do
    {:ok, vulnerabilities}
  end
  defp validate_request(_), do: {:error, :invalid_request}
  
  defp process_with_cache(forge_account_id, repository, vulnerabilities, files) do
    # Track cache statistics
    cache_stats = %{
      hits: 0,
      misses: 0,
      invalidated: 0,
      internal_duplicates: 0,
      total_requests: 0
    }
    
    # Track internal duplicates within this batch
    internal_cache = %{}
    
    {validated_results, final_stats, _} = 
      Enum.reduce(vulnerabilities, {[], cache_stats, internal_cache}, 
        fn vuln, {results, stats, batch_cache} ->
          
          # Extract locations and vulnerability type
          locations = extract_locations(vuln)
          vulnerability_type = vuln["type"]
          file_hashes = extract_file_hashes(vuln, files)
          
          # Generate cache key for internal deduplication
          cache_key = ValidationCache.KeyGenerator.generate_key(
            forge_account_id,
            repository,
            locations,
            vulnerability_type
          )
          
          stats = %{stats | total_requests: stats.total_requests + 1}
          
          # Check if we've already processed this in the current batch
          case Map.get(batch_cache, cache_key) do
            nil ->
              # Not in batch cache, check persistent cache
              case ValidationCache.get(forge_account_id, repository, locations, vulnerability_type, file_hashes) do
                {:ok, cached} ->
                  # Cache hit!
                  result = build_cached_result(vuln, cached, repository)
                  stats = %{stats | hits: stats.hits + 1}
                  batch_cache = Map.put(batch_cache, cache_key, result)
                  {[result | results], stats, batch_cache}
                  
                {:miss, nil} ->
                  # Cache miss - perform validation
                  result = perform_validation_and_cache(
                    vuln, files, forge_account_id, repository, locations, vulnerability_type, file_hashes
                  )
                  stats = %{stats | misses: stats.misses + 1}
                  batch_cache = Map.put(batch_cache, cache_key, result)
                  {[result | results], stats, batch_cache}
                  
                {:invalidated, nil} ->
                  # Cache was invalidated due to file change
                  result = perform_validation_and_cache(
                    vuln, files, forge_account_id, repository, locations, vulnerability_type, file_hashes
                  )
                  result = Map.put(result, "cacheInvalidatedReason", "file_changed")
                  stats = %{stats | invalidated: stats.invalidated + 1, misses: stats.misses + 1}
                  batch_cache = Map.put(batch_cache, cache_key, result)
                  {[result | results], stats, batch_cache}
                  
                {:expired, nil} ->
                  # Cache expired
                  result = perform_validation_and_cache(
                    vuln, files, forge_account_id, repository, locations, vulnerability_type, file_hashes
                  )
                  stats = %{stats | misses: stats.misses + 1}
                  batch_cache = Map.put(batch_cache, cache_key, result)
                  {[result | results], stats, batch_cache}
              end
              
            cached_result ->
              # Found in batch cache (internal duplicate)
              stats = %{stats | internal_duplicates: stats.internal_duplicates + 1}
              {[cached_result | results], stats, batch_cache}
          end
      end)
    
    {Enum.reverse(validated_results), final_stats}
  end
  
  defp extract_locations(vuln) do
    case vuln["locations"] do
      nil -> 
        # Fallback to old format
        [%{
          file_path: vuln["filePath"] || vuln["file_path"],
          line: vuln["line"] || 1
        }]
      locations when is_list(locations) ->
        Enum.map(locations, fn loc ->
          %{
            file_path: loc["file_path"] || loc["filePath"],
            line: loc["line"] || 1
          }
        end)
    end
  end
  
  defp extract_file_hashes(vuln, files) do
    locations = extract_locations(vuln)
    
    locations
    |> Enum.map(& &1.file_path)
    |> Enum.uniq()
    |> Map.new(fn path ->
      file_data = Map.get(files, path, %{})
      hash = case file_data do
        %{"hash" => hash} when is_binary(hash) -> hash
        _ -> "sha256:unknown"
      end
      {path, hash}
    end)
  end
  
  defp build_cached_result(vuln, cached, repository) do
    %{
      "id" => vuln["id"],
      "isValid" => not cached.is_false_positive,
      "confidence" => Decimal.to_float(cached.confidence),
      "reason" => cached.reason,
      "fromCache" => true,
      "cachedAt" => DateTime.to_iso8601(cached.cached_at),
      "cacheKey" => "#{cached.forge_account_id}/#{repository}/#{format_locations(cached.locations)}:#{cached.vulnerability_type}",
      "cacheScope" => "organization",
      "cacheHitType" => "exact_match",
      "astContext" => cached.full_result || %{}
    }
  end
  
  defp format_locations(locations) do
    locations
    |> Enum.map(fn loc -> "#{loc["file_path"]}:#{loc["line"]}" end)
    |> Enum.sort()
    |> Enum.join(",")
    |> then(&"[#{&1}]")
  end
  
  defp perform_validation_and_cache(vuln, files, forge_account_id, repository, locations, vulnerability_type, file_hashes) do
    # Perform the actual AST validation
    file_content = get_file_content(vuln, files)
    validation_result = perform_ast_validation(vuln, file_content)
    
    # Store in cache if it's a false positive
    if not validation_result["isValid"] do
      cache_data = %{
        forge_account_id: forge_account_id,
        repository: repository,
        locations: locations,
        vulnerability_type: vulnerability_type,
        file_hashes: file_hashes,
        is_false_positive: true,
        confidence: validation_result["confidence"] || 0.5,
        reason: validation_result["reason"],
        full_result: validation_result["astContext"]
      }
      
      case ValidationCache.store(cache_data) do
        {:ok, _cached} ->
          Logger.debug("Stored false positive in cache for #{vulnerability_type}")
        {:error, reason} ->
          Logger.warning("Failed to cache validation result: #{inspect(reason)}")
      end
    end
    
    # Return result without cache metadata
    Map.merge(validation_result, %{
      "fromCache" => false
    })
  end
  
  defp get_file_content(vuln, files) do
    file_path = vuln["filePath"] || vuln["file_path"]
    case Map.get(files, file_path) do
      %{"content" => content} -> content
      _ -> ""
    end
  end
  
  defp perform_ast_validation(vuln, file_content) do
    # This is a simplified version - copy the logic from the original controller
    # For testing, we'll classify most things as false positives to test caching
    file_classification = FilePathClassifier.classify(vuln["filePath"])
    confidence_multiplier = FilePathClassifier.confidence_multiplier(file_classification)

    vulnerability_type = vuln["type"] |> to_vulnerability_atom()
    language = detect_language(vuln["filePath"])

    is_safe = SafePatternDetector.is_safe_pattern?(
      vulnerability_type,
      file_content,
      %{language: language}
    )

    # Check if this is a safe pattern (false positive)
    if is_safe do
      explanation = SafePatternDetector.explain_safety(
        vulnerability_type,
        file_content,
        %{language: language}
      )
      
      %{
        "id" => vuln["id"],
        "isValid" => false,  # False positive - not a real vulnerability
        "confidence" => 0.95 * confidence_multiplier,
        "reason" => explanation,
        "astContext" => %{
          "inUserInputFlow" => false,
          "hasValidation" => true,
          "fileClassification" => to_string(file_classification),
          "safePattern" => is_safe
        }
      }
    else
      # Real vulnerability (not used in test mode)
      %{
        "id" => vuln["id"],
        "isValid" => true,
        "confidence" => 0.8 * confidence_multiplier,
        "reason" => nil,
        "astContext" => %{
          "inUserInputFlow" => true,
          "hasValidation" => false,
          "fileClassification" => to_string(file_classification)
        }
      }
    end
  end
  
  defp build_cache_stats(stats) do
    total = stats.total_requests
    hit_rate = if total > 0, do: stats.hits / total * 100, else: 0
    
    # Get additional cache metrics
    case ValidationCache.CachedValidation 
         |> Rsolv.Repo.aggregate(:count) do
      count ->
        %{
          "total_requests" => total,
          "cache_hits" => stats.hits,
          "cache_misses" => stats.misses,
          "internal_duplicates" => stats.internal_duplicates,
          "hit_rate" => Float.round(hit_rate, 2),
          "total_cached_entries" => count,
          "avg_ttl_remaining" => 89.5  # Approximate for now
        }
    end
  end
  
  defp calculate_stats(validated_results) do
    total = length(validated_results)
    validated = Enum.count(validated_results, fn r -> r["isValid"] == true end)
    rejected = total - validated
    
    %{
      "total" => total,
      "validated" => validated,
      "rejected" => rejected
    }
  end
  
  defp to_vulnerability_atom(type) when is_binary(type) do
    type
    |> String.downcase()
    |> String.replace("-", "_")
    |> String.to_atom()
  end
  defp to_vulnerability_atom(type), do: type
  
  defp detect_language(file_path) when is_binary(file_path) do
    cond do
      String.ends_with?(file_path, [".js", ".jsx", ".ts", ".tsx", ".mjs"]) -> "javascript"
      String.ends_with?(file_path, [".py"]) -> "python"
      String.ends_with?(file_path, [".rb", ".erb"]) -> "ruby"
      String.ends_with?(file_path, [".php"]) -> "php"
      String.ends_with?(file_path, [".ex", ".exs"]) -> "elixir"
      String.ends_with?(file_path, [".go"]) -> "go"
      String.ends_with?(file_path, [".java"]) -> "java"
      String.ends_with?(file_path, [".rs"]) -> "rust"
      true -> "javascript"
    end
  end
  defp detect_language(_), do: "javascript"
end